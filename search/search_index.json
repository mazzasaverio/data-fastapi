{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FastAPI Your Data Documentation Welcome to the official documentation for FastAPI Your Data, a project aimed at providing a comprehensive guide and toolkit for managing, organizing, and utilizing data effectively with FastAPI. Overview This documentation is designed to be both a theoretical guide and a practical handbook, serving as a technical diary for the considerations and experiments conducted throughout the development of the FastAPI Your Data project. Navigate through the sections to find detailed information on setup, usage, best practices, and insights gained from real-world application and continuous improvement of the project. Getting Started If you're new to FastAPI Your Data, we recommend starting with the Getting Started section to set up your environment and take your first steps with the project. Contributions Contributions to FastAPI Your Data are welcome! Thank you for being part of the FastAPI Your Data community!","title":"Home"},{"location":"#welcome-to-fastapi-your-data-documentation","text":"Welcome to the official documentation for FastAPI Your Data, a project aimed at providing a comprehensive guide and toolkit for managing, organizing, and utilizing data effectively with FastAPI.","title":"Welcome to FastAPI Your Data Documentation"},{"location":"#overview","text":"This documentation is designed to be both a theoretical guide and a practical handbook, serving as a technical diary for the considerations and experiments conducted throughout the development of the FastAPI Your Data project. Navigate through the sections to find detailed information on setup, usage, best practices, and insights gained from real-world application and continuous improvement of the project.","title":"Overview"},{"location":"#getting-started","text":"If you're new to FastAPI Your Data, we recommend starting with the Getting Started section to set up your environment and take your first steps with the project.","title":"Getting Started"},{"location":"#contributions","text":"Contributions to FastAPI Your Data are welcome! Thank you for being part of the FastAPI Your Data community!","title":"Contributions"},{"location":"changelog/","text":"","title":"Changelog"},{"location":"chapter-1-getting-started/installation/","text":"Setting Up Your Environment with Poetry To correctly set up your environment with Poetry inside the backend/app directory, follow these steps: 1. Install Poetry First, ensure Poetry is installed. If it's not already installed, you can do so by running the following command in your terminal: curl -sSL https://install.python-poetry.org | python3 - After installation, make sure to add Poetry to your system's PATH. 2. Configure Poetry Navigate to your project's root directory ( /home/sam/github/fastapi-your-data ) and then enter the backend/app directory. If you're using a custom setup or working within a specific part of your project, adjust paths accordingly. cd /home/sam/github/fastapi-your-data/backend/app Before proceeding, ensure you have a pyproject.toml file in the app directory. This file defines your project and its dependencies. If it's not present, you can create it by running: poetry init And follow the interactive prompts. 3. Install Dependencies poetry add pandas uvicorn fastapi pytest loguru pydantic-settings alembic pgvector This command installs all necessary packages in a virtual environment managed by Poetry. 4. Activate the Virtual Environment To activate the Poetry-managed virtual environment, use the following command: poetry shell This command spawns a shell within the virtual environment, allowing you to run your project's Python scripts and manage dependencies. 5. Running Your Application Within the activated virtual environment and the correct directory, you can run your FastAPI application. For instance, to run the main application found in backend/app/app/main.py , execute: uvicorn app.main:app --reload This command starts the Uvicorn server with hot reload enabled, serving your FastAPI application. 6. Deactivating the Virtual Environment When you're done working in the virtual environment, you can exit by typing exit or pressing Ctrl+D . Additional Tips Environment Variables : Make sure to set up any required environment variables. You can manage them using .env files and load them in your application using libraries like python-dotenv . Dependency Management : Use poetry add <package-name> to add new dependencies and poetry update to update existing ones. Testing and Linting : Utilize Poetry to run tests and linters by adding custom scripts in the pyproject.toml file. This guide should help you set up and manage your Python environment effectively using Poetry, enhancing your development workflow for the FastAPI project located in /home/sam/github/fastapi-your-data/backend/app .","title":"Installation"},{"location":"chapter-1-getting-started/installation/#setting-up-your-environment-with-poetry","text":"To correctly set up your environment with Poetry inside the backend/app directory, follow these steps:","title":"Setting Up Your Environment with Poetry"},{"location":"chapter-1-getting-started/installation/#1-install-poetry","text":"First, ensure Poetry is installed. If it's not already installed, you can do so by running the following command in your terminal: curl -sSL https://install.python-poetry.org | python3 - After installation, make sure to add Poetry to your system's PATH.","title":"1. Install Poetry"},{"location":"chapter-1-getting-started/installation/#2-configure-poetry","text":"Navigate to your project's root directory ( /home/sam/github/fastapi-your-data ) and then enter the backend/app directory. If you're using a custom setup or working within a specific part of your project, adjust paths accordingly. cd /home/sam/github/fastapi-your-data/backend/app Before proceeding, ensure you have a pyproject.toml file in the app directory. This file defines your project and its dependencies. If it's not present, you can create it by running: poetry init And follow the interactive prompts.","title":"2. Configure Poetry"},{"location":"chapter-1-getting-started/installation/#3-install-dependencies","text":"poetry add pandas uvicorn fastapi pytest loguru pydantic-settings alembic pgvector This command installs all necessary packages in a virtual environment managed by Poetry.","title":"3. Install Dependencies"},{"location":"chapter-1-getting-started/installation/#4-activate-the-virtual-environment","text":"To activate the Poetry-managed virtual environment, use the following command: poetry shell This command spawns a shell within the virtual environment, allowing you to run your project's Python scripts and manage dependencies.","title":"4. Activate the Virtual Environment"},{"location":"chapter-1-getting-started/installation/#5-running-your-application","text":"Within the activated virtual environment and the correct directory, you can run your FastAPI application. For instance, to run the main application found in backend/app/app/main.py , execute: uvicorn app.main:app --reload This command starts the Uvicorn server with hot reload enabled, serving your FastAPI application.","title":"5. Running Your Application"},{"location":"chapter-1-getting-started/installation/#6-deactivating-the-virtual-environment","text":"When you're done working in the virtual environment, you can exit by typing exit or pressing Ctrl+D .","title":"6. Deactivating the Virtual Environment"},{"location":"chapter-1-getting-started/installation/#additional-tips","text":"Environment Variables : Make sure to set up any required environment variables. You can manage them using .env files and load them in your application using libraries like python-dotenv . Dependency Management : Use poetry add <package-name> to add new dependencies and poetry update to update existing ones. Testing and Linting : Utilize Poetry to run tests and linters by adding custom scripts in the pyproject.toml file. This guide should help you set up and manage your Python environment effectively using Poetry, enhancing your development workflow for the FastAPI project located in /home/sam/github/fastapi-your-data/backend/app .","title":"Additional Tips"},{"location":"chapter-2-project-structure/project_structure/","text":"Project Structure and Microservice Design Patterns (FastAPI Your Data - Episode 1) A well-organized project structure is paramount for any development endeavor. The ideal structure is one that maintains consistency, simplicity, and predictability throughout. A clear project structure should immediately convey the essence of the project to anyone reviewing it. If it fails to do so, it may be considered ambiguous. The necessity to navigate through packages to decipher the contents and purpose of modules indicates a lack of clarity in the structure. An arbitrary arrangement of files, both in terms of frequency and location, signifies a poorly organized project structure. When the naming and placement of a module do not intuitively suggest its functionality, the structure is deemed highly ineffective. Exploring Microservice Design Patterns Our journey into the realm of software architecture focuses on strategies and principles designed to ease the transition from monolithic systems to a microservices architecture. This exploration is centered around breaking down a large application into smaller, more manageable segments, known as problem domains or use cases. A critical step in this process is the establishment of a unified gateway that consolidates these domains, thereby facilitating smoother interaction and integration. Additionally, we employ specialized modeling techniques tailored to each microservice, while also tackling essential aspects such as logging and configuration management within the application. Objectives and Topics The aim is to showcase the effectiveness and feasibility of applying these architectural patterns to a software sample. To accomplish this, we will explore several essential topics, including: Application of decomposition patterns Creation of a common gateway Centralization of logging mechanisms Consumption of REST APIs Application of domain modeling approaches Management of microservice configurations Principles on Project Structure There are numerous ways to structure a project, but the optimal structure is one that is consistent, straightforward, and devoid of surprises. If a glance at the project structure doesn't convey what the project entails, then the structure might be unclear. If you need to open packages to decipher the modules within them, your structure is not clear. If the organization and location of files seem arbitrary, then your project structure is poor. If the module's location and name don't offer insight into its contents, then the structure is very poor. Although the project structure, where files are separated by type (e.g., api, crud, models, schemas) as presented by @tiangolo, is suitable for microservices or projects with limited scopes, it was not adaptable to our monolith with numerous domains and modules. A structure I found more scalable and evolvable draws inspiration from Netflix's Dispatch, with some minor adjustments. Applying the Decomposition Pattern For instance, consider solving problems like: Identifying GitHub repositories that match your interests and where you'd like to contribute, based on a questionnaire that analyzes similarities with repository READMEs and other details. A module for finding companies, startups, or projects that align with your personality and characteristics. A note organization and interaction module. A module that guides you through decision-making questions based on books of interest. Each microservice operates independently, having its server instance, management, logging mechanism, dependencies, container, and configuration file. Starting or shutting down one service does not affect the others, thanks to unique context roots and ports. Sub-Applications in FastAPI FastAPI provides an alternative design approach through the creation of sub-applications within a main application. The main application file ( main.py ) acts as a gateway, directing traffic to these sub-applications based on their context paths. This setup allows for the mounting of FastAPI instances for each sub-application, showcasing FastAPI's flexibility in microservice design. Sub-applications, such as sub_app_1 , sub_app_2 , etc., are typical independent microservice applications mounted into the main.py component, the top-level application. Each sub-application has a main.py component which sets up its FastAPI instance, as demonstrated in the following code snippet: from fastapi import FastAPI sub_app_1 = FastAPI() sub_app_1.include_router(admin.router) sub_app_1.include_router(management.router) These sub-applications are typical FastAPI microservice applications containing all essential components such as routers, middleware, exception handlers, and all necessary packages to build REST API services. The only difference from standard applications is that their context paths or URLs are defined and managed by the top-level application that oversees them. Optionally, we can run sub-applications independently from main.py using commands like uvicorn main:sub_app_1 --port 8001 for sub_app_1 , uvicorn main:sub_app_2 --port 8082 for sub_app_2 , and uvicorn main:sub_app_3 --port 8003 for sub_app_3 . The ability to run them independently despite being mounted illustrates why these mounted sub-applications are considered microservices. Mounting Submodules All FastAPI decorators from each sub-application must be mounted in the main.py component of the top-level application to be accessible at runtime. The mount() function is called by the FastAPI decorator object of the top-level application, which incorporates all FastAPI instances of the sub-applications into the gateway application ( main.py ) and assigns each to its corresponding URL context. from fastapi import FastAPI from application.sub_app_1 import main as sub_app_1_main app = FastAPI() app.mount(\"/sub_app_1\", sub_app_1_main.app) With this configuration, the mounted /sub_app_1 URL will be used to access all the API services of the sub_app_1 module app. These mounted paths are recognized once declared in mount() , as FastAPI automatically manages all these paths through the root_path specification. Since all sub-applications in our system are independent microservices, let's now apply another design strategy to manage requests to these applications using only the main URL. We will use the main application as a gateway to our sub-applications. Creating a Common Gateway It will be simpler to use the URL of the main application to manage requests and direct users to any sub-application. The main application can act as a pseudo-reverse proxy or an entry point for user requests, which will then redirect user requests to the desired sub-application. This approach is based on a design pattern known as API Gateway. Let's explore how we can implement this design to manage independent microservices mounted on the main application using a workaround. Implementing the Main Endpoint There are various solutions for implementing this gateway endpoint. One option is to create a simple REST API service in the top-level application with an integer path parameter that identifies the microservice's ID parameter. If the ID parameter is invalid, the endpoint will return a JSON string instead of an error. Below is a straightforward implementation of this endpoint: from fastapi import APIRouter router = APIRouter() @router.get(\"/platform/{portal_id}\") def access_portal(portal_id: int): return {\"message\": \"Welcome\"} The access_portal API endpoint is established as a GET path operation with portal_id as its path parameter. This parameter is crucial because it determines which sub-app microservice the user wishes to access. Evaluating the Microservice ID The portal_id parameter is automatically retrieved and evaluated using a dependable function injected into the APIRouter instance where the API endpoint is defined. from fastapi import Request def call_api_gateway(request: Request): portal_id = request.path_params[\"portal_id\"] print(request.path_params) if portal_id == str(1): raise RedirectSubApp1PortalException() class RedirectSubApp1PortalException(Exception): pass Evaluating the Microservice ID This solution is a practical workaround to initiate a custom event, as FastAPI lacks built-in event handling aside from startup and shutdown event handlers. Once call_api_gateway() identifies portal_id as a valid microservice ID, it will raise custom exceptions. For instance, it will throw RedirectStudentPortalException if the user aims to access a specific microservice. However, first, we need to inject call_api_gateway() into the APIRouter instance managing the gateway endpoint through the main.py component of the top-level application. from fastapi import FastAPI, Depends, Request, Response from gateway.api_router import call_api_gateway from controller import platform app = FastAPI() app.include_router(platform.router, dependencies=[Depends(call_api_gateway)]) All raised exceptions require an exception handler to listen for the throws and execute necessary tasks to engage with the microservices. References and Inspirations Github fastapi-alembic-sqlmodel-async : A project integrating FastAPI with Alembic and SQLModel for asynchronous database operations. fastcrud : A library for simplifying CRUD operations in FastAPI. agentkit : A toolkit for building intelligent agents. fastapi-best-practices Books Building Python Microservices with FastAPI","title":"Project structure"},{"location":"chapter-2-project-structure/project_structure/#project-structure-and-microservice-design-patterns-fastapi-your-data-episode-1","text":"A well-organized project structure is paramount for any development endeavor. The ideal structure is one that maintains consistency, simplicity, and predictability throughout. A clear project structure should immediately convey the essence of the project to anyone reviewing it. If it fails to do so, it may be considered ambiguous. The necessity to navigate through packages to decipher the contents and purpose of modules indicates a lack of clarity in the structure. An arbitrary arrangement of files, both in terms of frequency and location, signifies a poorly organized project structure. When the naming and placement of a module do not intuitively suggest its functionality, the structure is deemed highly ineffective.","title":"Project Structure and Microservice Design Patterns (FastAPI Your Data - Episode 1)"},{"location":"chapter-2-project-structure/project_structure/#exploring-microservice-design-patterns","text":"Our journey into the realm of software architecture focuses on strategies and principles designed to ease the transition from monolithic systems to a microservices architecture. This exploration is centered around breaking down a large application into smaller, more manageable segments, known as problem domains or use cases. A critical step in this process is the establishment of a unified gateway that consolidates these domains, thereby facilitating smoother interaction and integration. Additionally, we employ specialized modeling techniques tailored to each microservice, while also tackling essential aspects such as logging and configuration management within the application.","title":"Exploring Microservice Design Patterns"},{"location":"chapter-2-project-structure/project_structure/#objectives-and-topics","text":"The aim is to showcase the effectiveness and feasibility of applying these architectural patterns to a software sample. To accomplish this, we will explore several essential topics, including: Application of decomposition patterns Creation of a common gateway Centralization of logging mechanisms Consumption of REST APIs Application of domain modeling approaches Management of microservice configurations","title":"Objectives and Topics"},{"location":"chapter-2-project-structure/project_structure/#principles-on-project-structure","text":"There are numerous ways to structure a project, but the optimal structure is one that is consistent, straightforward, and devoid of surprises. If a glance at the project structure doesn't convey what the project entails, then the structure might be unclear. If you need to open packages to decipher the modules within them, your structure is not clear. If the organization and location of files seem arbitrary, then your project structure is poor. If the module's location and name don't offer insight into its contents, then the structure is very poor. Although the project structure, where files are separated by type (e.g., api, crud, models, schemas) as presented by @tiangolo, is suitable for microservices or projects with limited scopes, it was not adaptable to our monolith with numerous domains and modules. A structure I found more scalable and evolvable draws inspiration from Netflix's Dispatch, with some minor adjustments.","title":"Principles on Project Structure"},{"location":"chapter-2-project-structure/project_structure/#applying-the-decomposition-pattern","text":"For instance, consider solving problems like: Identifying GitHub repositories that match your interests and where you'd like to contribute, based on a questionnaire that analyzes similarities with repository READMEs and other details. A module for finding companies, startups, or projects that align with your personality and characteristics. A note organization and interaction module. A module that guides you through decision-making questions based on books of interest. Each microservice operates independently, having its server instance, management, logging mechanism, dependencies, container, and configuration file. Starting or shutting down one service does not affect the others, thanks to unique context roots and ports.","title":"Applying the Decomposition Pattern"},{"location":"chapter-2-project-structure/project_structure/#sub-applications-in-fastapi","text":"FastAPI provides an alternative design approach through the creation of sub-applications within a main application. The main application file ( main.py ) acts as a gateway, directing traffic to these sub-applications based on their context paths. This setup allows for the mounting of FastAPI instances for each sub-application, showcasing FastAPI's flexibility in microservice design. Sub-applications, such as sub_app_1 , sub_app_2 , etc., are typical independent microservice applications mounted into the main.py component, the top-level application. Each sub-application has a main.py component which sets up its FastAPI instance, as demonstrated in the following code snippet: from fastapi import FastAPI sub_app_1 = FastAPI() sub_app_1.include_router(admin.router) sub_app_1.include_router(management.router)","title":"Sub-Applications in FastAPI"},{"location":"chapter-2-project-structure/project_structure/#_1","text":"These sub-applications are typical FastAPI microservice applications containing all essential components such as routers, middleware, exception handlers, and all necessary packages to build REST API services. The only difference from standard applications is that their context paths or URLs are defined and managed by the top-level application that oversees them. Optionally, we can run sub-applications independently from main.py using commands like uvicorn main:sub_app_1 --port 8001 for sub_app_1 , uvicorn main:sub_app_2 --port 8082 for sub_app_2 , and uvicorn main:sub_app_3 --port 8003 for sub_app_3 . The ability to run them independently despite being mounted illustrates why these mounted sub-applications are considered microservices.","title":""},{"location":"chapter-2-project-structure/project_structure/#mounting-submodules","text":"All FastAPI decorators from each sub-application must be mounted in the main.py component of the top-level application to be accessible at runtime. The mount() function is called by the FastAPI decorator object of the top-level application, which incorporates all FastAPI instances of the sub-applications into the gateway application ( main.py ) and assigns each to its corresponding URL context. from fastapi import FastAPI from application.sub_app_1 import main as sub_app_1_main app = FastAPI() app.mount(\"/sub_app_1\", sub_app_1_main.app) With this configuration, the mounted /sub_app_1 URL will be used to access all the API services of the sub_app_1 module app. These mounted paths are recognized once declared in mount() , as FastAPI automatically manages all these paths through the root_path specification. Since all sub-applications in our system are independent microservices, let's now apply another design strategy to manage requests to these applications using only the main URL. We will use the main application as a gateway to our sub-applications.","title":"Mounting Submodules"},{"location":"chapter-2-project-structure/project_structure/#creating-a-common-gateway","text":"It will be simpler to use the URL of the main application to manage requests and direct users to any sub-application. The main application can act as a pseudo-reverse proxy or an entry point for user requests, which will then redirect user requests to the desired sub-application. This approach is based on a design pattern known as API Gateway. Let's explore how we can implement this design to manage independent microservices mounted on the main application using a workaround.","title":"Creating a Common Gateway"},{"location":"chapter-2-project-structure/project_structure/#implementing-the-main-endpoint","text":"There are various solutions for implementing this gateway endpoint. One option is to create a simple REST API service in the top-level application with an integer path parameter that identifies the microservice's ID parameter. If the ID parameter is invalid, the endpoint will return a JSON string instead of an error. Below is a straightforward implementation of this endpoint: from fastapi import APIRouter router = APIRouter() @router.get(\"/platform/{portal_id}\") def access_portal(portal_id: int): return {\"message\": \"Welcome\"} The access_portal API endpoint is established as a GET path operation with portal_id as its path parameter. This parameter is crucial because it determines which sub-app microservice the user wishes to access.","title":"Implementing the Main Endpoint"},{"location":"chapter-2-project-structure/project_structure/#evaluating-the-microservice-id","text":"The portal_id parameter is automatically retrieved and evaluated using a dependable function injected into the APIRouter instance where the API endpoint is defined. from fastapi import Request def call_api_gateway(request: Request): portal_id = request.path_params[\"portal_id\"] print(request.path_params) if portal_id == str(1): raise RedirectSubApp1PortalException() class RedirectSubApp1PortalException(Exception): pass","title":"Evaluating the Microservice ID"},{"location":"chapter-2-project-structure/project_structure/#evaluating-the-microservice-id_1","text":"This solution is a practical workaround to initiate a custom event, as FastAPI lacks built-in event handling aside from startup and shutdown event handlers. Once call_api_gateway() identifies portal_id as a valid microservice ID, it will raise custom exceptions. For instance, it will throw RedirectStudentPortalException if the user aims to access a specific microservice. However, first, we need to inject call_api_gateway() into the APIRouter instance managing the gateway endpoint through the main.py component of the top-level application. from fastapi import FastAPI, Depends, Request, Response from gateway.api_router import call_api_gateway from controller import platform app = FastAPI() app.include_router(platform.router, dependencies=[Depends(call_api_gateway)]) All raised exceptions require an exception handler to listen for the throws and execute necessary tasks to engage with the microservices.","title":"Evaluating the Microservice ID"},{"location":"chapter-2-project-structure/project_structure/#references-and-inspirations","text":"","title":"References and Inspirations"},{"location":"chapter-2-project-structure/project_structure/#github","text":"fastapi-alembic-sqlmodel-async : A project integrating FastAPI with Alembic and SQLModel for asynchronous database operations. fastcrud : A library for simplifying CRUD operations in FastAPI. agentkit : A toolkit for building intelligent agents. fastapi-best-practices","title":"Github"},{"location":"chapter-2-project-structure/project_structure/#books","text":"Building Python Microservices with FastAPI","title":"Books"},{"location":"chapter-3-database/vector_databases/","text":"","title":"Vector databases"},{"location":"chapter-4-data-migration/alembic/","text":"Enhanced Alembic Migrations Documentation Migration Naming Convention When creating migration scripts, adhere to a consistent naming scheme that reflects the nature of the changes. The recommended format is date_slug.py , where date represents the creation date and slug provides a brief description of the migration's purpose. For example, 2022-08-24_post_content_idx.py . Configuration File Template In the alembic.ini file, define the file template to ensure that all migration scripts follow the established naming convention. The template should look like this: file_template = %%(year)d-%%(month).2d-%%(day).2d_%%(slug)s This ensures that every new migration script includes the date and a descriptive slug. Automated Migration Script Generation To streamline the process of creating migration scripts, use the --autogenerate option with the alembic revision command. This feature compares the current state of your database schema with your SQLAlchemy models and generates a migration script accordingly. Here's how you can use it: alembic revision --autogenerate -m \"Create tables\" Replace \"Create tables\" with a message that accurately describes the changes being implemented. Applying Migrations After generating the migration scripts, apply them to your database with the alembic upgrade command followed by head . This command executes the migration scripts against your database, bringing it up to date with the latest schema changes. alembic upgrade head By following these practices, you maintain a clear and organized history of your database schema changes, making it easier to manage and understand the evolution of your data model over time.","title":"Alembic"},{"location":"chapter-4-data-migration/alembic/#enhanced-alembic-migrations-documentation","text":"","title":"Enhanced Alembic Migrations Documentation"},{"location":"chapter-4-data-migration/alembic/#migration-naming-convention","text":"When creating migration scripts, adhere to a consistent naming scheme that reflects the nature of the changes. The recommended format is date_slug.py , where date represents the creation date and slug provides a brief description of the migration's purpose. For example, 2022-08-24_post_content_idx.py .","title":"Migration Naming Convention"},{"location":"chapter-4-data-migration/alembic/#configuration-file-template","text":"In the alembic.ini file, define the file template to ensure that all migration scripts follow the established naming convention. The template should look like this: file_template = %%(year)d-%%(month).2d-%%(day).2d_%%(slug)s This ensures that every new migration script includes the date and a descriptive slug.","title":"Configuration File Template"},{"location":"chapter-4-data-migration/alembic/#automated-migration-script-generation","text":"To streamline the process of creating migration scripts, use the --autogenerate option with the alembic revision command. This feature compares the current state of your database schema with your SQLAlchemy models and generates a migration script accordingly. Here's how you can use it: alembic revision --autogenerate -m \"Create tables\" Replace \"Create tables\" with a message that accurately describes the changes being implemented.","title":"Automated Migration Script Generation"},{"location":"chapter-4-data-migration/alembic/#applying-migrations","text":"After generating the migration scripts, apply them to your database with the alembic upgrade command followed by head . This command executes the migration scripts against your database, bringing it up to date with the latest schema changes. alembic upgrade head By following these practices, you maintain a clear and organized history of your database schema changes, making it easier to manage and understand the evolution of your data model over time.","title":"Applying Migrations"},{"location":"chapter-5-iaac/terraform/","text":"1. Google Cloud Platform Account Sign Up : Ensure you have an active GCP account. Sign up here if needed. 2. Project Setup New Project : Create a new GCP project. Note down the project ID for future use. 3. Service Account Create Service Account : Create a service account with 'Owner' permissions in your GCP project. Generate Key File : Generate a JSON key file for this service account and store it securely. 4. Billing Enable Billing : Ensure billing is enabled on your GCP project for using paid services. 5. Connecting Cloud Build to Your GitHub Account Create a personal access token. Make sure to set your token (classic) to have no expiration date and select the following permissions when prompted in GitHub: repo and read:user. If your app is installed in an organization, make sure to also select the read:org permission. https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github?generation=2nd-gen#terraform_1 Terraform Configuration Rename File : Change terraform.tfvars.example to terraform.tfvars . Insert Credentials : Add your credentials to the terraform.tfvars file. Connecting to Cloud SQL using Cloud SQL Proxy (Example with DBeaver) For a secure connection to your Cloud SQL instance from local development environments or database management tools like DBeaver, the Cloud SQL Proxy provides a robust solution. Follow these steps to set up and use the Cloud SQL Proxy: Download the Cloud SQL Proxy : Use the command below to download the latest version of Cloud SQL Proxy for Linux: bash curl -o cloud-sql-proxy https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.8.1/cloud-sql-proxy.linux.amd64 Make the Proxy Executable : Change the downloaded file's permissions to make it executable: bash chmod +x cloud-sql-proxy Start the Cloud SQL Proxy : Launch the proxy with your Cloud SQL instance details. Replace the [INSTANCE_CONNECTION_NAME] with your specific Cloud SQL instance connection name: bash ./cloud-sql-proxy --credentials-file=/path/to/credentials_file.json 'project-name:region:instance-name?port=port_number' Connect using DBeaver : Open DBeaver and create a new database connection. Set the host to localhost and the port to 5433 (or the port you specified). Provide your Cloud SQL instance's database credentials. For more details on using the Cloud SQL Proxy, visit the official documentation: Google Cloud SQL Proxy Documentation Useful Commands Perform only a few modules (attention to addictions) : bash terraform apply -target=module.compute_instance Add SSH Key : bash ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Connect via SSH : bash ssh -i /path/to/your/private/key your_instance_username@external_ip_address bash chmod 600 ~/.ssh/id_rsa Test Cloud SQL Connection : bash psql -h private_ip_address -U database_user -d database_name Additional Information For detailed implementation, refer to the contents of specific .tf files within each module's directory.","title":"Terraform"},{"location":"chapter-5-iaac/terraform/#1-google-cloud-platform-account","text":"Sign Up : Ensure you have an active GCP account. Sign up here if needed.","title":"1. Google Cloud Platform Account"},{"location":"chapter-5-iaac/terraform/#2-project-setup","text":"New Project : Create a new GCP project. Note down the project ID for future use.","title":"2. Project Setup"},{"location":"chapter-5-iaac/terraform/#3-service-account","text":"Create Service Account : Create a service account with 'Owner' permissions in your GCP project. Generate Key File : Generate a JSON key file for this service account and store it securely.","title":"3. Service Account"},{"location":"chapter-5-iaac/terraform/#4-billing","text":"Enable Billing : Ensure billing is enabled on your GCP project for using paid services.","title":"4. Billing"},{"location":"chapter-5-iaac/terraform/#5-connecting-cloud-build-to-your-github-account","text":"Create a personal access token. Make sure to set your token (classic) to have no expiration date and select the following permissions when prompted in GitHub: repo and read:user. If your app is installed in an organization, make sure to also select the read:org permission. https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github?generation=2nd-gen#terraform_1","title":"5. Connecting Cloud Build to Your GitHub Account"},{"location":"chapter-5-iaac/terraform/#terraform-configuration","text":"Rename File : Change terraform.tfvars.example to terraform.tfvars . Insert Credentials : Add your credentials to the terraform.tfvars file.","title":"Terraform Configuration"},{"location":"chapter-5-iaac/terraform/#connecting-to-cloud-sql-using-cloud-sql-proxy-example-with-dbeaver","text":"For a secure connection to your Cloud SQL instance from local development environments or database management tools like DBeaver, the Cloud SQL Proxy provides a robust solution. Follow these steps to set up and use the Cloud SQL Proxy: Download the Cloud SQL Proxy : Use the command below to download the latest version of Cloud SQL Proxy for Linux: bash curl -o cloud-sql-proxy https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.8.1/cloud-sql-proxy.linux.amd64 Make the Proxy Executable : Change the downloaded file's permissions to make it executable: bash chmod +x cloud-sql-proxy Start the Cloud SQL Proxy : Launch the proxy with your Cloud SQL instance details. Replace the [INSTANCE_CONNECTION_NAME] with your specific Cloud SQL instance connection name: bash ./cloud-sql-proxy --credentials-file=/path/to/credentials_file.json 'project-name:region:instance-name?port=port_number' Connect using DBeaver : Open DBeaver and create a new database connection. Set the host to localhost and the port to 5433 (or the port you specified). Provide your Cloud SQL instance's database credentials. For more details on using the Cloud SQL Proxy, visit the official documentation: Google Cloud SQL Proxy Documentation","title":"Connecting to Cloud SQL using Cloud SQL Proxy (Example with DBeaver)"},{"location":"chapter-5-iaac/terraform/#useful-commands","text":"Perform only a few modules (attention to addictions) : bash terraform apply -target=module.compute_instance Add SSH Key : bash ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Connect via SSH : bash ssh -i /path/to/your/private/key your_instance_username@external_ip_address bash chmod 600 ~/.ssh/id_rsa Test Cloud SQL Connection : bash psql -h private_ip_address -U database_user -d database_name","title":"Useful Commands"},{"location":"chapter-5-iaac/terraform/#additional-information","text":"For detailed implementation, refer to the contents of specific .tf files within each module's directory.","title":"Additional Information"},{"location":"chapter-6-linux/linux/","text":"verificare cosa occupa spazio su linux du -a . | sort -n -r | head -n 10","title":"Linux"},{"location":"chapter-journal/episode2/","text":"A step back and pgvector for \u2026 (FastAPI Your Data\u200a-\u200aEpisode 2) In this second episode of our project journey, I've made significant changes to the backend structure. The decision to focus primarily on a streamlined version means that what will eventually evolve into sub-applications are currently functioning as API routes. This approach is driven by the desire to first achieve a complete backend and frontend integration for a single sub-application. Despite not having a detailed plan at this stage, the myriad of topics needing exploration, study, and testing makes a rigid plan less practical. For instance, today, I spent more time than anticipated deciding on a vector database, eventually settling on pgvector for seamless integration with PostgreSQL. The significance of vector databases will become more apparent as we delve into modules involving natural language processing. GitHub Projects and Repos Search One of the modules I'm excited to introduce and test involves searching GitHub projects and repositories based on criteria beyond the filters GitHub offers. I aim to enable searches based on project descriptions and README files, assigning scores based on my skills, goals, and interests. To this end, I've started with a basic script that fetches READMEs from GitHub users, extracts the content, and performs embedding, as shown below: import requests from app.api.v1.endpoints.utils.github_api import GitHubAPI from tqdm import tqdm from loguru import logger def fetch_users_by_location(location, max_users, access_token): users = [] url = f\"https://api.github.com/search/users?q=location:{location}&per_page={max_users}\" headers = {\"Authorization\": f\"token {access_token}\"} try: response = requests.get(url, headers=headers).json() users.extend(response.get(\"items\", [])) logger.info(f\"Successfully fetched {len(users)} users from location: {location}\") except Exception as e: logger.error(f\"Failed to fetch users from location: {location}. Error: {e}\") return users GitHub Projects and Repos Search 2 This functionality is currently integrated as one of the endpoints, and I'm curious to see where it leads. My plan for tomorrow includes deploying everything on Cloud Run, though I must first address PostgreSQL initialization with pgvector, which requires enabling the extension\u2014a task that is proving to be challenging directly through SQLAlchemy. def fetch_repo_readmes(users, max_repos_per_user, access_token): github_api = GitHubAPI(access_token) all_readmes = [] for user in tqdm(users, desc=\"Fetching repositories\"): repos = github_api.get_user_repos(user[\"login\"], max_repos=max_repos_per_user) for repo in repos: readme = github_api.get_readme(repo) if readme: all_readmes.append( { \"username\": user[\"login\"], \"repo_name\": repo[\"name\"], \"readme_text\": readme, } ) return all_readmes Cloud Deployment and SQL Challenges For deploying on Cloud SQL in production through Cloud Run, I've encountered a workaround that involves using an SQL file for extension creation, which, while not the most elegant solution, serves our immediate needs. The importance of elegance is currently low on the priority list. I've found guidance for Cloud SQL integration with pgvector here , but further investigation is needed. This episode marks another step forward in our project, blending backend development with vector databases and AI applications. Stay tuned for more updates as we continue to navigate through these exciting challenges and opportunities.","title":"Episode2"},{"location":"chapter-journal/episode2/#a-step-back-and-pgvector-for-fastapi-your-data-episode-2","text":"In this second episode of our project journey, I've made significant changes to the backend structure. The decision to focus primarily on a streamlined version means that what will eventually evolve into sub-applications are currently functioning as API routes. This approach is driven by the desire to first achieve a complete backend and frontend integration for a single sub-application. Despite not having a detailed plan at this stage, the myriad of topics needing exploration, study, and testing makes a rigid plan less practical. For instance, today, I spent more time than anticipated deciding on a vector database, eventually settling on pgvector for seamless integration with PostgreSQL. The significance of vector databases will become more apparent as we delve into modules involving natural language processing.","title":"A step back and pgvector for\u00a0\u2026 (FastAPI Your Data\u200a-\u200aEpisode\u00a02)"},{"location":"chapter-journal/episode2/#github-projects-and-repos-search","text":"One of the modules I'm excited to introduce and test involves searching GitHub projects and repositories based on criteria beyond the filters GitHub offers. I aim to enable searches based on project descriptions and README files, assigning scores based on my skills, goals, and interests. To this end, I've started with a basic script that fetches READMEs from GitHub users, extracts the content, and performs embedding, as shown below: import requests from app.api.v1.endpoints.utils.github_api import GitHubAPI from tqdm import tqdm from loguru import logger def fetch_users_by_location(location, max_users, access_token): users = [] url = f\"https://api.github.com/search/users?q=location:{location}&per_page={max_users}\" headers = {\"Authorization\": f\"token {access_token}\"} try: response = requests.get(url, headers=headers).json() users.extend(response.get(\"items\", [])) logger.info(f\"Successfully fetched {len(users)} users from location: {location}\") except Exception as e: logger.error(f\"Failed to fetch users from location: {location}. Error: {e}\") return users","title":"GitHub Projects and Repos Search"},{"location":"chapter-journal/episode2/#github-projects-and-repos-search-2","text":"This functionality is currently integrated as one of the endpoints, and I'm curious to see where it leads. My plan for tomorrow includes deploying everything on Cloud Run, though I must first address PostgreSQL initialization with pgvector, which requires enabling the extension\u2014a task that is proving to be challenging directly through SQLAlchemy. def fetch_repo_readmes(users, max_repos_per_user, access_token): github_api = GitHubAPI(access_token) all_readmes = [] for user in tqdm(users, desc=\"Fetching repositories\"): repos = github_api.get_user_repos(user[\"login\"], max_repos=max_repos_per_user) for repo in repos: readme = github_api.get_readme(repo) if readme: all_readmes.append( { \"username\": user[\"login\"], \"repo_name\": repo[\"name\"], \"readme_text\": readme, } ) return all_readmes","title":"GitHub Projects and Repos Search 2"},{"location":"chapter-journal/episode2/#cloud-deployment-and-sql-challenges","text":"For deploying on Cloud SQL in production through Cloud Run, I've encountered a workaround that involves using an SQL file for extension creation, which, while not the most elegant solution, serves our immediate needs. The importance of elegance is currently low on the priority list. I've found guidance for Cloud SQL integration with pgvector here , but further investigation is needed. This episode marks another step forward in our project, blending backend development with vector databases and AI applications. Stay tuned for more updates as we continue to navigate through these exciting challenges and opportunities.","title":"Cloud Deployment and SQL Challenges"},{"location":"chapter-journal/episode3/","text":"Database Initialization and pgvector Extension I've refined the initialization process for the pgvector vector database as follows: async def init_db() -> None: create_database( settings.DB_NAME, settings.DB_USER, settings.DB_PASS, settings.DB_HOST, settings.DB_PORT, ) # Ensure the vector extension is created after initializing the database await create_extension() logger.info(\"Vector extension creation check attempted.\") async_engine = create_async_engine(settings.ASYNC_DATABASE_URI, echo=True) async with async_engine.begin() as conn: await conn.run_sync(Base.metadata.create_all) logger.info(\"Database initialized and all tables created if they didn't exist.\") 1 By leveraging the following: @asynccontextmanager async def app_lifespan(app: FastAPI): await init_db() yield app = FastAPI(lifespan=app_lifespan) This approach internalizes the process within the code, eliminating the need for workarounds and optimizing the setup. 2 I've also modified the Dockerfile as follows to avoid issues with model loading each time the application is launched: # Install necessary dependencies RUN pip install sentence-transformers # Pre-download the model to the image RUN python -c \"from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')\" This setup will be replaced with a solution where the model is either loaded from a volume or the results are directly returned via an API (we'll test OpenAI's embedding models). 3 Additionally, I plan to explore another extension integrated with PostgreSQL, pg_embedding , as seen below: As hinted at in the previous episode, the next step involves setting up Cloud Run connected to Cloud SQL for PostgreSQL. Stay tuned.","title":"Episode3"},{"location":"chapter-journal/episode3/#database-initialization-and-pgvector-extension","text":"I've refined the initialization process for the pgvector vector database as follows: async def init_db() -> None: create_database( settings.DB_NAME, settings.DB_USER, settings.DB_PASS, settings.DB_HOST, settings.DB_PORT, ) # Ensure the vector extension is created after initializing the database await create_extension() logger.info(\"Vector extension creation check attempted.\") async_engine = create_async_engine(settings.ASYNC_DATABASE_URI, echo=True) async with async_engine.begin() as conn: await conn.run_sync(Base.metadata.create_all) logger.info(\"Database initialized and all tables created if they didn't exist.\")","title":"Database Initialization and pgvector Extension"},{"location":"chapter-journal/episode3/#1","text":"By leveraging the following: @asynccontextmanager async def app_lifespan(app: FastAPI): await init_db() yield app = FastAPI(lifespan=app_lifespan) This approach internalizes the process within the code, eliminating the need for workarounds and optimizing the setup.","title":"1"},{"location":"chapter-journal/episode3/#2","text":"I've also modified the Dockerfile as follows to avoid issues with model loading each time the application is launched: # Install necessary dependencies RUN pip install sentence-transformers # Pre-download the model to the image RUN python -c \"from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')\" This setup will be replaced with a solution where the model is either loaded from a volume or the results are directly returned via an API (we'll test OpenAI's embedding models).","title":"2"},{"location":"chapter-journal/episode3/#3","text":"Additionally, I plan to explore another extension integrated with PostgreSQL, pg_embedding , as seen below: As hinted at in the previous episode, the next step involves setting up Cloud Run connected to Cloud SQL for PostgreSQL. Stay tuned.","title":"3"},{"location":"tutorials/how-to-create-a-changelog/","text":"Structuring a Changelog and Keeping It Updated Introduction to Changelog A changelog is a file that contains a curated, chronologically ordered list of notable changes for each version of a project. It serves to document the progress and significant changes made over time, making it easier for users and contributors to track what has been added, changed, removed, or fixed. How to Structure a Changelog File Naming and Format : The changelog file should be named CHANGELOG.md to indicate that it is written in Markdown format. This allows for easy readability and formatting. Release Headings : Each version release should have its own section, starting with a second-level heading that includes the version number and release date in the format ## [Version] - YYYY-MM-DD . It's essential to follow Semantic Versioning rules when versioning your releases. Change Categories : Within each release section, group changes into categories to improve readability. Common categories include: Added for new features. Changed for changes in existing functionality. Deprecated for soon-to-be-removed features. Removed for now-removed features. Fixed for any bug fixes. Security to address vulnerabilities. Notable Changes : List the changes under their respective categories using bullet points. Each item should briefly describe the change and, if applicable, reference the issue or pull request number. For example: Added: New search functionality to allow users to find posts by tags (#123). Highlighting Breaking Changes : Clearly highlight any breaking changes or migrations required by the users. This could be done in a separate section or marked distinctly within the appropriate category. Keeping Changelog Updated Manual Updates : Manually update the changelog as part of your project's release process. This involves summarizing the changes made since the last release, categorizing them, and adding them to the CHANGELOG.md file. Automate Generation : For projects with well-structured commit messages or pull requests, you can automate changelog generation using tools like GitHub Changelog Generator . These tools can parse your project's history to create a changelog draft that you can then edit for clarity and readability. Commit Message Discipline : Adopting a convention for commit messages, such as Conventional Commits , can facilitate the automated generation of changelogs. This approach requires discipline in writing commit messages but pays off in automation capabilities. Review Process : Regardless of whether the changelog is manually updated or generated automatically, it's essential to review the changelog entries for accuracy, clarity, and relevance to the project's users before finalizing a release. Example Changelog Entry ## [1.2.0] - 2024-03-10 ### Added - New search functionality to allow users to find posts by tags (#123). ### Changed - Improved performance of the database query for fetching posts. ### Fixed - Fixed a bug where users could not reset their passwords (#456). ### Security - Patched XSS vulnerability in post comments section. ### Breaking Changes - Removed deprecated `getPostById` API method. Use `getPost` instead. Conclusion Maintaining a changelog is a best practice that benefits both the project's users and its developers. By structuring the changelog clearly and keeping it up to date with each release, you ensure that your project's progress is transparent and that users can easily understand the impact of each update.","title":"How to Create a Changelog"},{"location":"tutorials/how-to-create-a-changelog/#structuring-a-changelog-and-keeping-it-updated","text":"","title":"Structuring a Changelog and Keeping It Updated"},{"location":"tutorials/how-to-create-a-changelog/#introduction-to-changelog","text":"A changelog is a file that contains a curated, chronologically ordered list of notable changes for each version of a project. It serves to document the progress and significant changes made over time, making it easier for users and contributors to track what has been added, changed, removed, or fixed.","title":"Introduction to Changelog"},{"location":"tutorials/how-to-create-a-changelog/#how-to-structure-a-changelog","text":"File Naming and Format : The changelog file should be named CHANGELOG.md to indicate that it is written in Markdown format. This allows for easy readability and formatting. Release Headings : Each version release should have its own section, starting with a second-level heading that includes the version number and release date in the format ## [Version] - YYYY-MM-DD . It's essential to follow Semantic Versioning rules when versioning your releases. Change Categories : Within each release section, group changes into categories to improve readability. Common categories include: Added for new features. Changed for changes in existing functionality. Deprecated for soon-to-be-removed features. Removed for now-removed features. Fixed for any bug fixes. Security to address vulnerabilities. Notable Changes : List the changes under their respective categories using bullet points. Each item should briefly describe the change and, if applicable, reference the issue or pull request number. For example: Added: New search functionality to allow users to find posts by tags (#123). Highlighting Breaking Changes : Clearly highlight any breaking changes or migrations required by the users. This could be done in a separate section or marked distinctly within the appropriate category.","title":"How to Structure a Changelog"},{"location":"tutorials/how-to-create-a-changelog/#keeping-changelog-updated","text":"Manual Updates : Manually update the changelog as part of your project's release process. This involves summarizing the changes made since the last release, categorizing them, and adding them to the CHANGELOG.md file. Automate Generation : For projects with well-structured commit messages or pull requests, you can automate changelog generation using tools like GitHub Changelog Generator . These tools can parse your project's history to create a changelog draft that you can then edit for clarity and readability. Commit Message Discipline : Adopting a convention for commit messages, such as Conventional Commits , can facilitate the automated generation of changelogs. This approach requires discipline in writing commit messages but pays off in automation capabilities. Review Process : Regardless of whether the changelog is manually updated or generated automatically, it's essential to review the changelog entries for accuracy, clarity, and relevance to the project's users before finalizing a release.","title":"Keeping Changelog Updated"},{"location":"tutorials/how-to-create-a-changelog/#example-changelog-entry","text":"## [1.2.0] - 2024-03-10 ### Added - New search functionality to allow users to find posts by tags (#123). ### Changed - Improved performance of the database query for fetching posts. ### Fixed - Fixed a bug where users could not reset their passwords (#456). ### Security - Patched XSS vulnerability in post comments section. ### Breaking Changes - Removed deprecated `getPostById` API method. Use `getPost` instead.","title":"Example Changelog Entry"},{"location":"tutorials/how-to-create-a-changelog/#conclusion","text":"Maintaining a changelog is a best practice that benefits both the project's users and its developers. By structuring the changelog clearly and keeping it up to date with each release, you ensure that your project's progress is transparent and that users can easily understand the impact of each update.","title":"Conclusion"},{"location":"tutorials/mkdocs-setup-and-usage-guide/","text":"Creating a Documentation with MkDocs To create a living documentation for your project fastapi-your-data that acts as a theoretical guide, practical handbook, and technical diary, we will use MkDocs in combination with GitHub and Python. This guide covers setting up MkDocs, organizing documentation, configuring it with mkdocs.yml , writing documentation in Markdown, and deploying it using GitHub Actions. Setting Up MkDocs Install MkDocs : Ensure you have Python 3.6 or higher and pip installed. Install MkDocs with pip: pip install mkdocs Initialize MkDocs Project : In your project's root directory ( /home/sam/github/fastapi-your-data ), initialize MkDocs: mkdocs new . This creates a mkdocs.yml configuration file and a docs directory with an index.md file for your documentation. Organizing Documentation Content Create a structured documentation within the docs directory. Example structure: docs/ index.md getting-started/ installation.md tutorials/ mkdocs-setup-and-usage-guide.md changelog.md Configuring Documentation in mkdocs.yml Edit the mkdocs.yml file to define your documentation's structure and navigation. Example configuration: site_name: FastAPI Your Data Documentation nav: - Home: index.md - Getting Started: getting-started/installation.md - Tutorials: tutorials/mkdocs-setup-and-usage-guide.md - Changelog: changelog.md theme: readthedocs Writing Documentation Write your documentation content in Markdown format. Markdown files should be saved inside the docs directory according to the structure defined in mkdocs.yml . Example content for installation.md : # Installation ## Requirements - Python 3.6 or higher - pip ## Installation Steps To install the required packages, run: ```bash pip install -r requirements.txt ``` #### Previewing Documentation Locally Use MkDocs' built-in server to preview your documentation: ```bash mkdocs serve Visit http://127.0.0.1:8080 in your browser to see your documentation. Deploying Documentation Build the static site with: mkdocs build The static site is generated in the site directory. Deploy this directory to any web server. For GitHub Pages, you can automate deployment using GitHub Actions as described below. Automating Deployment with GitHub Actions GitHub Actions Workflow In your project, create a workflow file under .github/workflows/ (e.g., deploy-docs.yml ) to define the steps for building and deploying your documentation to GitHub Pages. Generating a GitHub Token To perform actions such as deploying to GitHub Pages through GitHub Actions, you often need a GitHub token with the appropriate permissions. Here's how you can generate a MY_GITHUB_TOKEN : Access GitHub Token Settings Log in to your GitHub account. Click on your profile picture in the top right corner and select Settings . On the left sidebar, click on Developer settings . Under Developer settings, click on Personal access tokens . Click on the Generate new token button. Configure Token Permissions Give your token a descriptive name in the Note field. Set the expiration for your token as per your requirement. For continuous integration (CI) purposes, you might want to select a longer duration or no expiration. Select the scopes or permissions you want to grant this token. For deploying to GitHub Pages, you typically need: repo - Full control of private repositories (includes public_repo for public repositories). Additionally, you might need other permissions based on your specific requirements, but for deployment, repo is often sufficient. Scroll down and click Generate token . After clicking Generate token , GitHub will display your new personal access token. Make sure to copy your new personal access token now. You won\u2019t be able to see it again! For use in GitHub Actions: Go to your repository on GitHub. Click on Settings > Secrets > Actions . Click on New repository secret . Name your secret MY_GITHUB_TOKEN (or another name if you prefer, but remember to reference the correct name in your workflow file). Paste your token into the Value field and click Add secret .``` If you named your secret something other than MY_GITHUB_TOKEN , make sure to reference it correctly in the MY_GITHUB_TOKEN field. Workflow Example Here's an example workflow that uses the peaceiris/actions-gh-pages action to deploy your MkDocs site to GitHub Pages: name: Deploy MkDocs Site on: push: branches: - master jobs: deploy-docs: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Setup Python uses: actions/setup-python@v2 with: python-version: \"3.x\" - name: Install dependencies run: | pip install mkdocs - name: Build MkDocs site run: mkdocs build - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.MY_GITHUB_TOKEN }} publish_dir: ./site This workflow automatically builds and deploys your MkDocs site to GitHub Pages whenever changes are pushed to the master branch. Conclusion You've now set up MkDocs for your project, organized your documentation, written content in Markdown, previewed it locally, and deployed it using GitHub Actions. This setup allows you to maintain a comprehensive, up-to-date documentation for your project, facilitating both development and user guidance.","title":"MkDocs Setup and Usage Guide"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#creating-a-documentation-with-mkdocs","text":"To create a living documentation for your project fastapi-your-data that acts as a theoretical guide, practical handbook, and technical diary, we will use MkDocs in combination with GitHub and Python. This guide covers setting up MkDocs, organizing documentation, configuring it with mkdocs.yml , writing documentation in Markdown, and deploying it using GitHub Actions.","title":"Creating a Documentation with MkDocs"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#setting-up-mkdocs","text":"Install MkDocs : Ensure you have Python 3.6 or higher and pip installed. Install MkDocs with pip: pip install mkdocs Initialize MkDocs Project : In your project's root directory ( /home/sam/github/fastapi-your-data ), initialize MkDocs: mkdocs new . This creates a mkdocs.yml configuration file and a docs directory with an index.md file for your documentation.","title":"Setting Up MkDocs"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#organizing-documentation-content","text":"Create a structured documentation within the docs directory. Example structure: docs/ index.md getting-started/ installation.md tutorials/ mkdocs-setup-and-usage-guide.md changelog.md","title":"Organizing Documentation Content"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#configuring-documentation-in-mkdocsyml","text":"Edit the mkdocs.yml file to define your documentation's structure and navigation. Example configuration: site_name: FastAPI Your Data Documentation nav: - Home: index.md - Getting Started: getting-started/installation.md - Tutorials: tutorials/mkdocs-setup-and-usage-guide.md - Changelog: changelog.md theme: readthedocs","title":"Configuring Documentation in mkdocs.yml"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#writing-documentation","text":"Write your documentation content in Markdown format. Markdown files should be saved inside the docs directory according to the structure defined in mkdocs.yml . Example content for installation.md : # Installation ## Requirements - Python 3.6 or higher - pip ## Installation Steps To install the required packages, run: ```bash pip install -r requirements.txt ``` #### Previewing Documentation Locally Use MkDocs' built-in server to preview your documentation: ```bash mkdocs serve Visit http://127.0.0.1:8080 in your browser to see your documentation.","title":"Writing Documentation"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#deploying-documentation","text":"Build the static site with: mkdocs build The static site is generated in the site directory. Deploy this directory to any web server. For GitHub Pages, you can automate deployment using GitHub Actions as described below.","title":"Deploying Documentation"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#automating-deployment-with-github-actions","text":"GitHub Actions Workflow In your project, create a workflow file under .github/workflows/ (e.g., deploy-docs.yml ) to define the steps for building and deploying your documentation to GitHub Pages. Generating a GitHub Token To perform actions such as deploying to GitHub Pages through GitHub Actions, you often need a GitHub token with the appropriate permissions. Here's how you can generate a MY_GITHUB_TOKEN : Access GitHub Token Settings Log in to your GitHub account. Click on your profile picture in the top right corner and select Settings . On the left sidebar, click on Developer settings . Under Developer settings, click on Personal access tokens . Click on the Generate new token button. Configure Token Permissions Give your token a descriptive name in the Note field. Set the expiration for your token as per your requirement. For continuous integration (CI) purposes, you might want to select a longer duration or no expiration. Select the scopes or permissions you want to grant this token. For deploying to GitHub Pages, you typically need: repo - Full control of private repositories (includes public_repo for public repositories). Additionally, you might need other permissions based on your specific requirements, but for deployment, repo is often sufficient. Scroll down and click Generate token . After clicking Generate token , GitHub will display your new personal access token. Make sure to copy your new personal access token now. You won\u2019t be able to see it again! For use in GitHub Actions: Go to your repository on GitHub. Click on Settings > Secrets > Actions . Click on New repository secret . Name your secret MY_GITHUB_TOKEN (or another name if you prefer, but remember to reference the correct name in your workflow file). Paste your token into the Value field and click Add secret .``` If you named your secret something other than MY_GITHUB_TOKEN , make sure to reference it correctly in the MY_GITHUB_TOKEN field. Workflow Example Here's an example workflow that uses the peaceiris/actions-gh-pages action to deploy your MkDocs site to GitHub Pages: name: Deploy MkDocs Site on: push: branches: - master jobs: deploy-docs: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Setup Python uses: actions/setup-python@v2 with: python-version: \"3.x\" - name: Install dependencies run: | pip install mkdocs - name: Build MkDocs site run: mkdocs build - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.MY_GITHUB_TOKEN }} publish_dir: ./site This workflow automatically builds and deploys your MkDocs site to GitHub Pages whenever changes are pushed to the master branch.","title":"Automating Deployment with GitHub Actions"},{"location":"tutorials/mkdocs-setup-and-usage-guide/#conclusion","text":"You've now set up MkDocs for your project, organized your documentation, written content in Markdown, previewed it locally, and deployed it using GitHub Actions. This setup allows you to maintain a comprehensive, up-to-date documentation for your project, facilitating both development and user guidance.","title":"Conclusion"}]}